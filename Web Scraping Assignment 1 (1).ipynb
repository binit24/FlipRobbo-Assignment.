{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book-Page Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebox and Chrome.\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Link:\n",
    "link = \"https://bookpage.com/reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing the variable:\n",
    "page = requests.get(link)\n",
    "print(\"Total Responses:\",page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the tags having Book Title:\n",
    "title = soup.find_all('div',class_='flex-article-content')\n",
    "\n",
    "#Creating a list-comprehension to store the tags of Book Titles:\n",
    "Book_Name = []\n",
    "for i in range(0,len(title)):\n",
    "    Book_Name.append(title[i].get_text())\n",
    "print(list(Book_Name)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "Book_Name[:] = [title.strip('\\n').split(\"\\n\\n\\n\")[:][0].replace(\" â˜… \",\"\") for title in Book_Name]\n",
    "print(Book_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(Book_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the tags having author name:\n",
    "author = soup.find_all('div',class_='flex-article-content')\n",
    "\n",
    "#Creating a list-comprehension to store the tags of Book Titles:\n",
    "author_name = []\n",
    "for i in range(0,len(author)):\n",
    "    author_name.append(title[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "author_name[:] = [author.lower().strip('\\n').split(\"\\n\\n\\n\")[:][1] for author in author_name]\n",
    "print(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the tags having genre:\n",
    "genre = soup.find_all('div',class_='flex-article-content')\n",
    "\n",
    "#Creating a list-comprehension to store the tags of Book Titles:\n",
    "genres = []\n",
    "for i in range(0,len(genre)):\n",
    "    genres.append(genre[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "genres[:] = [genre.lower().strip('\\n').split(\"\\n\\n\\n\")[:][2].replace(\"\\n / \\n\",\"-\") for genre in genres]\n",
    "print(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Pandas:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame:\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the data into Dataframes:\n",
    "df['Books Names'] = Book_Name\n",
    "df['Author Names'] = author_name\n",
    "df['genres'] = genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataframe:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICC Men Cricket Cricket Ranking Web Scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list:\n",
    "Cricket_teams = []\n",
    "Total_matches = []\n",
    "Total_points = []\n",
    "Total_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having cricket teams.\n",
    "team = soup.find_all('span',class_='u-hide-phablet')\n",
    "for i in team:\n",
    "    Cricket_teams.append(i.text.strip())\n",
    "    Cricket_teams = Cricket_teams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Cricket_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total matches played.\n",
    "matches = soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "for i in matches:\n",
    "    Total_matches.append(i.text.strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see.\n",
    "matches = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in range(0,len(matches),2): #We are going use one index from matches and using odd ones from Matches.\n",
    "    Total_matches.append(matches[i].text)\n",
    "    Total_matches = Total_matches[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total Points as per matches.\n",
    "points = soup.find_all('td',class_='rankings-block__banner--points')\n",
    "for i in points:\n",
    "    Total_points.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "points = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in range(1,len(matches),2):\n",
    "    Total_points.append(points[i].text)\n",
    "    Total_points = Total_points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total ratings as per matches.\n",
    "ratings = soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "for i in ratings:\n",
    "    Total_ratings.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "ratings = soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "for i in ratings:\n",
    "    Total_ratings.append(i.text.replace('\\n','').strip())\n",
    "    Total_ratings = Total_ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ODI = pd.DataFrame({})\n",
    "ODI['teams'] = Cricket_teams\n",
    "ODI['matches'] = Total_matches\n",
    "ODI['points'] = Total_points\n",
    "ODI['ratings'] = Total_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 ODI Men Bowlers with ratings and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create blank list:\n",
    "players_name = []\n",
    "teams = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players names.\n",
    "player = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in player:\n",
    "    players_name.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "player = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in player:\n",
    "    players_name.append(i.find('a').text)\n",
    "    players_name = players_name[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(players_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players teams.\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "team = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n','').strip())\n",
    "    teams = teams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players ratings.\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "rating = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "    ratings = ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Bowlers_ODI = pd.DataFrame({})\n",
    "Bowlers_ODI['players'] = players_name\n",
    "Bowlers_ODI['teams'] = teams\n",
    "Bowlers_ODI['ratings'] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "Bowlers_ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICC Women Cricket Cricket Ranking Web Scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we will be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list:\n",
    "Cricket_teams = []\n",
    "Total_matches = []\n",
    "Total_points = []\n",
    "Total_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having cricket teams.\n",
    "team = soup.find_all('span',class_='u-hide-phablet')\n",
    "for i in team:\n",
    "    Cricket_teams.append(i.text.strip())\n",
    "    Cricket_teams = Cricket_teams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Cricket_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total matches played.\n",
    "matches = soup.find_all('td',class_='rankings-block__banner--matches')\n",
    "for i in matches:\n",
    "    Total_matches.append(i.text.strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see.\n",
    "matches = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in range(0,len(matches),2): #We are going use one index from matches and using odd ones from Matches.\n",
    "    Total_matches.append(matches[i].text)\n",
    "    Total_matches = Total_matches[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total Points as per matches.\n",
    "points = soup.find_all('td',class_='rankings-block__banner--points')\n",
    "for i in points:\n",
    "    Total_points.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "points = soup.find_all('td',class_='table-body__cell u-center-text')\n",
    "for i in range(1,len(matches),2):\n",
    "    Total_points.append(points[i].text)\n",
    "    Total_points = Total_points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having total ratings as per matches.\n",
    "ratings = soup.find_all('td',class_='rankings-block__banner--rating u-text-right')\n",
    "for i in ratings:\n",
    "    Total_ratings.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "ratings = soup.find_all('td',class_='table-body__cell u-text-right rating')\n",
    "for i in ratings:\n",
    "    Total_ratings.append(i.text.replace('\\n','').strip())\n",
    "    Total_ratings = Total_ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(Total_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ODI = pd.DataFrame({})\n",
    "ODI['teams'] = Cricket_teams\n",
    "ODI['matches'] = Total_matches\n",
    "ODI['points'] = Total_points\n",
    "ODI['ratings'] = Total_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Women ODI Batsmen along with their records and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an empty list:\n",
    "players = []\n",
    "teams = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having player names.\n",
    "player_name = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in player_name:\n",
    "    players.append(i.text.strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "player_name = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in player_name:\n",
    "    players.append(i.text.strip())\n",
    "    players = players[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(\"Top 10 ODI Women batsman name are:\",players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having Team names.\n",
    "team_names = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team_names:\n",
    "    teams.append(i.text.strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "team_names = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team_names:\n",
    "    teams.append(i.text.strip())\n",
    "    teams = teams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players rating.\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text.strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see\n",
    "rating = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text.strip())\n",
    "    ratings = ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Women_batsmen_ODI = pd.DataFrame({})\n",
    "Women_batsmen_ODI['players'] = players\n",
    "Women_batsmen_ODI['teams'] = teams\n",
    "Women_batsmen_ODI['ratings'] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "Women_batsmen_ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 ODI Women Bowlers with ratings and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create blank list:\n",
    "players_name = []\n",
    "teams = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players names.\n",
    "player = soup.find_all('div',class_='rankings-block__banner--name-large')\n",
    "for i in player:\n",
    "    players_name.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "player = soup.find_all('td',class_='table-body__cell rankings-table__name name')\n",
    "for i in player:\n",
    "    players_name.append(i.find('a').text)\n",
    "    players_name = players_name[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(players_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players teams.\n",
    "team = soup.find_all('div',class_='rankings-block__banner--nationality')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "team = soup.find_all('span',class_='table-body__logo-text')\n",
    "for i in team:\n",
    "    teams.append(i.text.replace('\\n','').strip())\n",
    "    teams = teams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having players ratings.\n",
    "rating = soup.find_all('div',class_='rankings-block__banner--rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "    \n",
    "#The Top row html tag is different from other other 9 rows so lets see:\n",
    "rating = soup.find_all('td',class_='table-body__cell rating')\n",
    "for i in rating:\n",
    "    ratings.append(i.text)\n",
    "    ratings = ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the results:\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Women_ODI_Bowlers = pd.DataFrame({})\n",
    "Women_ODI_Bowlers['players'] = players_name\n",
    "Women_ODI_Bowlers['teams'] = teams\n",
    "Women_ODI_Bowlers['ratings'] = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "Women_ODI_Bowlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of IMBD Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create empty list.\n",
    "movie_titles = []\n",
    "release_date = []\n",
    "user_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having movie_titles.\n",
    "titles = soup.find_all('td',class_=\"titleColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the movie:\n",
    "movie_titles = []\n",
    "for i in range(0,len(titles)):\n",
    "    movie_titles.append(titles[i].get_text())\n",
    "movie_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "movie_titles[:] = [titles.replace(\"\\n\",\"\").strip('\\n').split(',')[-1] for titles in movie_titles]\n",
    "movie_titles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having release dates.\n",
    "year = soup.find_all('span',class_='secondaryInfo')\n",
    "year[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store release dates:\n",
    "release_date = []\n",
    "for i in range(0,len(year)):\n",
    "    release_date.append(year[i].get_text())\n",
    "release_date[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "release_date[:] = [year.strip() for year in release_date]\n",
    "release_date[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having user ratings.\n",
    "ratings = soup.find_all('td',class_=\"ratingColumn imdbRating\")\n",
    "ratings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store movie ratings:\n",
    "user_ratings = []\n",
    "for i in range(0,len(ratings)):\n",
    "    user_ratings.append(ratings[i].get_text())\n",
    "user_ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "user_ratings[:] = [ratings.replace(\"\\n\",\"\").strip()for ratings in user_ratings]\n",
    "user_ratings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies = pd.DataFrame({})\n",
    "movies['title'] = movie_titles[0:101]\n",
    "movies['release'] = release_date[0:101]\n",
    "movies['rating'] = user_ratings[0:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Index:\n",
    "movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "print(movies.shape)\n",
    "movies.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV File:\n",
    "import csv\n",
    "file_name = \"IMBD-Movies\"\n",
    "with open(file_name,'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sr.NO','Movies Name','Release Date','Movie Rating'])\n",
    "    for i in range(len(movie_titles)):\n",
    "        writer.writerow([i,movie_titles[i],release_date[i],user_ratings[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of Mobile Phones from Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebox and Chrome.\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.amazon.in/s?i=electronics&bbn=1805560031&rh=n%3A976419031%2Cn%3A976420031%2Cn%3A1389401031%2Cn%3A1389432031%2Cn%3A1805560031%2Cp_85%3A10440599031%2Cp_36%3A1318507031&dc&hidden-keywords=smartphone&pf_rd_i=1389401031&pf_rd_m=A1K21FY43GMZF8&pf_rd_p=2044db61-4e9b-4b45-be42-c23ae0223180&pf_rd_r=XWPMDQXX7GNZ663EFSM7&pf_rd_s=merchandised-search-20&pf_rd_t=101&qid=1610135393&rnid=1318502031&ref=sr_nr_p_36_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list:\n",
    "Names = []\n",
    "Price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags for names.\n",
    "for i in soup.find_all('a',class_=\"a-link-normal a-text-normal\"):\n",
    "    string = i.text\n",
    "    Names.append(string.strip() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the results:\n",
    "print(Names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags for Mobile.\n",
    "for i in soup.find_all('span',class_=\"a-price-whole\"):\n",
    "    string = i.text\n",
    "    Price.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the price results:\n",
    "print(Price[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Pandas:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataframe:\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the data into Dataframes:\n",
    "df['Mobile Name'] = Names\n",
    "df['Mobile Price'] = Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataframe:\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV File:\n",
    "import csv\n",
    "file_name = \"Mobile_Phones\"\n",
    "with open(file_name,'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sr.NO','Laptop Name','Price'])\n",
    "    for i in range(len(Names)):\n",
    "        writer.writerow([i,Names[i],Price[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.We are Going to Scrap only customer reviews from Amazon.in for Mobile Phones in new page.\n",
    "\n",
    "2.Here we are only going to extract customer reviews,names and ratings.\n",
    "\n",
    "3.We are only going to take 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.amazon.in/OnePlus-Mirror-Black-128GB-Storage/product-reviews/B07DJHV6VZ/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing the variable:\n",
    "page = requests.get(link)\n",
    "print(\"Total Responses:\",page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the customer review tags:\n",
    "names = soup.find_all('span',class_='a-profile-name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the result:\n",
    "list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the tags of customer names:\n",
    "customer_Name = []\n",
    "for i in range(0,len(names)):\n",
    "    customer_Name.append(names[i].get_text())\n",
    "print(list(customer_Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the names which are repeated:\n",
    "print(set([x for x in customer_Name if customer_Name.count(x)>1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate records:\n",
    "customer_Name.pop(0)\n",
    "print(list(customer_Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicate records:\n",
    "customer_Name.pop(0)\n",
    "print(list(customer_Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the customer review titles:\n",
    "titles = soup.find_all('a',class_='review-title-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the result:\n",
    "list(titles)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the customer review_titles:\n",
    "review_titles = []\n",
    "for i in range(0,len(titles)):\n",
    "    review_titles.append(titles[i].get_text())\n",
    "print(list(review_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "review_titles[:] = [titles.strip('\\n') for titles in review_titles]\n",
    "review_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the customer ratings:\n",
    "rating = soup.find_all('i',class_='review-rating')\n",
    "print(list(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the tags of customer rating:\n",
    "customer_ratings = []\n",
    "for i in range(0,len(rating)):\n",
    "    customer_ratings.append(rating[i].get_text())\n",
    "print(list(customer_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicate records:\n",
    "print(set([x for x in customer_ratings if customer_ratings.count(x)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicate Records:\n",
    "customer_ratings.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Duplicate Records:\n",
    "customer_ratings.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the length:\n",
    "len(customer_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the customer review body:\n",
    "review = soup.find_all(\"span\",{\"data-hook\":\"review-body\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the tags of customer review body:\n",
    "customer_review_body = []\n",
    "for i in range(0,len(review)):\n",
    "    customer_review_body.append(review[i].get_text())\n",
    "customer_review_body[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "customer_review_body[:] = [review.strip('\\n') for review in customer_review_body] \n",
    "customer_review_body[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Pandas:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DataFrame:\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the data into Dataframes:\n",
    "df['Customer Name'] = customer_Name\n",
    "df['review titles'] = review_titles\n",
    "df['Customer Ratings'] = customer_ratings\n",
    "df['review'] = customer_review_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataframe:\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of IMBD-World Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create empty list.\n",
    "movie_titles = []\n",
    "release_date = []\n",
    "user_ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having movie_titles.\n",
    "titles = soup.find_all('td',class_=\"titleColumn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the movie:\n",
    "movie_titles = []\n",
    "for i in range(0,len(titles)):\n",
    "    movie_titles.append(titles[i].get_text())\n",
    "movie_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "movie_titles[:] = [titles.replace(\"\\n\",\"\").strip('\\n').split(',')[-1] for titles in movie_titles]\n",
    "movie_titles[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having release dates.\n",
    "year = soup.find_all('span',class_='secondaryInfo')\n",
    "year[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store release dates:\n",
    "release_date = []\n",
    "for i in range(0,len(year)):\n",
    "    release_date.append(year[i].get_text())\n",
    "release_date[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "release_date[:] = [year.strip() for year in release_date]\n",
    "release_date[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having user ratings.\n",
    "ratings = soup.find_all('td',class_=\"ratingColumn imdbRating\")\n",
    "ratings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store movie ratings:\n",
    "user_ratings = []\n",
    "for i in range(0,len(ratings)):\n",
    "    user_ratings.append(ratings[i].get_text())\n",
    "user_ratings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "user_ratings[:] = [ratings.replace(\"\\n\",\"\").strip()for ratings in user_ratings]\n",
    "user_ratings[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing DataFrame:\n",
    "import pandas as pd\n",
    "movies = pd.DataFrame({})\n",
    "movies['title'] = movie_titles[0:101]\n",
    "movies['release'] = release_date[0:101]\n",
    "movies['rating'] = user_ratings[0:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Index:\n",
    "movies.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "print(movies.shape)\n",
    "movies.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to CSV File:\n",
    "import csv\n",
    "file_name = \"IMBD-Movies_World\"\n",
    "with open(file_name,'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sr.NO','Movies Name','Release Date','Movie Rating'])\n",
    "    for i in range(len(movie_titles)):\n",
    "        writer.writerow([i,movie_titles[i],release_date[i],user_ratings[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Job web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebox and Chrome.\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.monster.com/jobs/search/?q=Data-Scientist&where=New-Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create empty list.\n",
    "company_name = []\n",
    "jobs_titles = []\n",
    "job_location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having company name.\n",
    "names = soup.find_all('span',class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the company name:\n",
    "company_name = []\n",
    "for i in range(0,len(names)):\n",
    "    company_name.append(names[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "company_name[:] = [names.replace(\"\\r\",\"\").strip() for names in company_name]\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having job title.\n",
    "titles = soup.find_all('h2',class_=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the job title.\n",
    "jobs_titles = []\n",
    "for i in range(0,len(titles)):\n",
    "    jobs_titles.append(titles[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "jobs_titles[:] = [titles.replace(\"\\r\",\"\").strip() for titles in jobs_titles]\n",
    "jobs_titles = jobs_titles[1:]\n",
    "jobs_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(jobs_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having job location.\n",
    "locations = soup.find_all('span',class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the company location:\n",
    "job_location = []\n",
    "for i in range(0,len(locations)):\n",
    "    job_location.append(locations[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "job_location[:] = [locations.replace(\"\\r\",\"\").strip() for locations in job_location]\n",
    "job_location = job_location[1:]\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(job_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing DataFrame:\n",
    "import pandas as pd\n",
    "job = pd.DataFrame({})\n",
    "job['Company Name'] = company_name[0:24]\n",
    "job['Job Title'] = jobs_titles[0:24]\n",
    "job['job Location'] = job_location[0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Index:\n",
    "job.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "print(job.shape)\n",
    "job.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monster Jobportal Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebox and Chrome.\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting the request for webpage server:\n",
    "page = requests.get(\"https://www.monster.com/jobs/search/?q=Software-Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we wwill be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create empty list.\n",
    "company_name = []\n",
    "jobs_titles = []\n",
    "job_location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having company name.\n",
    "names = soup.find_all('span',class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the company name:\n",
    "company_name = []\n",
    "for i in range(0,len(names)):\n",
    "    company_name.append(names[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "company_name[:] = [names.replace(\"\\r\",\"\").strip() for names in company_name]\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(company_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having job title.\n",
    "titles = soup.find_all('h2',class_=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the job title.\n",
    "jobs_titles = []\n",
    "for i in range(0,len(titles)):\n",
    "    jobs_titles.append(titles[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "jobs_titles[:] = [titles.replace(\"\\r\",\"\").strip() for titles in jobs_titles]\n",
    "jobs_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(jobs_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract all the tags having job location.\n",
    "locations = soup.find_all('span',class_=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list-comprehension to store the company location:\n",
    "job_location = []\n",
    "for i in range(0,len(locations)):\n",
    "    job_location.append(locations[i].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the text:\n",
    "job_location[:] = [locations.replace(\"\\r\",\"\").strip() for locations in job_location]\n",
    "job_location = job_location[1:]\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the Length:\n",
    "len(job_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing DataFrame:\n",
    "import pandas as pd\n",
    "job = pd.DataFrame({})\n",
    "job['Company Name'] = company_name[0:24]\n",
    "job['Job Title'] = jobs_titles[0:24]\n",
    "job['job Location'] = job_location[0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting Index:\n",
    "job.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "print(job.shape)\n",
    "job.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia page web Scrapping for header tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firebox and Chrome.\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Microsoft Edge.\n",
    "#!pip install msedge-selenium-tools selenium==3.141\n",
    "from msedge.selenium_tools import Edge,EdgeOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nows lets check the download source code file:\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets parse the souce code by using Beautifull soup.\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print the title.\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For better visualization we will be using Prettify method.\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the header tags:\n",
    "titles = soup.find_all(['h1', 'h2','h3','h4','h5'])\n",
    "print('List all the header tags present :', *titles, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
