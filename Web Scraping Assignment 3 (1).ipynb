{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Write a python program which searches all the product under a particular product vertical from www.amazon.in. The product verticals to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.in/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search : Guitar\n"
     ]
    }
   ],
   "source": [
    "#User Input:\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Search Options:\n",
    "search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")    \n",
    "search_bar.clear()                                              \n",
    "search_bar.send_keys(user_inp)                                  \n",
    "search_button = driver.find_element_by_xpath('//div[@class=\"nav-search-submit nav-sprite\"]/span/input')      \n",
    "search_button.click()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the list are ==>> 147 144 147\n",
      "                                               fruits  \\\n",
      "0   https://www.healthyeating.org/Healthy-Eating/A...   \n",
      "1   https://www.amazon.in/Fruits-All-One-Dreamland...   \n",
      "2                 https://en.wikipedia.org/wiki/Fruit   \n",
      "3   https://www.pinterest.com/pin/777363585661725897/   \n",
      "4    https://www.diagnosisdiet.com/full-article/fruit   \n",
      "..                                                ...   \n",
      "95  https://www.npr.org/sections/thesalt/2019/07/1...   \n",
      "96  https://www.scientificamerican.com/article/are...   \n",
      "97  https://www.atlasobscura.com/articles/unusual-...   \n",
      "98  https://www.healthyeating.org/Healthy-Eating/A...   \n",
      "99  https://www.amazon.in/Fruits-All-One-Dreamland...   \n",
      "\n",
      "                                                 cars  \\\n",
      "0   https://www.forbes.com/sites/jimgorzelany/2019...   \n",
      "1   https://www.wired.com/story/jd-power-korean-ca...   \n",
      "2   https://auto.economictimes.indiatimes.com/news...   \n",
      "3   https://www.extremetech.com/extreme/303740-car...   \n",
      "4   https://www.caranddriver.com/features/g1538313...   \n",
      "..                                                ...   \n",
      "95  https://m.economictimes.com/wealth/spend/best-...   \n",
      "96  https://www.forbes.com/sites/jimgorzelany/2019...   \n",
      "97  https://www.wired.com/story/jd-power-korean-ca...   \n",
      "98  https://auto.economictimes.indiatimes.com/news...   \n",
      "99  https://www.extremetech.com/extreme/303740-car...   \n",
      "\n",
      "                                     machine_learning  \n",
      "0   https://www.forbes.com/sites/tomtaulli/2019/03...  \n",
      "1   https://becominghuman.ai/an-introduction-to-ma...  \n",
      "2   https://www.securityroundtable.org/the-growing...  \n",
      "3     https://www.geeksforgeeks.org/machine-learning/  \n",
      "4   https://www.wordstream.com/blog/ws/2017/07/28/...  \n",
      "..                                                ...  \n",
      "95  https://blog.bismart.com/en/difference-between...  \n",
      "96  https://www.nokia.com/blog/study-ai-machine-le...  \n",
      "97  https://www.devprojournal.com/technology-trend...  \n",
      "98  https://www.forbes.com/sites/tomtaulli/2019/03...  \n",
      "99  https://becominghuman.ai/an-introduction-to-ma...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function Definition\n",
    "def google_img(url):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    fruits=[]\n",
    "    cars=[]\n",
    "    machine_learning=[]\n",
    "#extracting fruits data\n",
    "    driver.get(url)\n",
    "    search_field_items=driver.find_element_by_xpath(\"//div[@class='a4bIc']/input\")\n",
    "    search_field_items.send_keys(\"Bikes\")\n",
    "    search_button=driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\")\n",
    "    search_button.click()\n",
    "    for i in range(0,3):\n",
    "        url_tags=driver.find_elements_by_xpath(\"//div[@class='islrc']//div[@class='isv-r PNCib MSM1fd BUooTd']//a[@class='VFACy kGQAp sMi44c lNHeqe WGvvNb']\")\n",
    "        for i in url_tags:\n",
    "            fruits.append(i.get_attribute('href'))   \n",
    "        time.sleep(2)\n",
    "#extracting car image data        \n",
    "    driver.get(url)\n",
    "    search_field_items=driver.find_element_by_xpath(\"//div[@class='a4bIc']/input\")\n",
    "    search_field_items.send_keys(\"cars\")\n",
    "    search_button=driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\")\n",
    "    search_button.click()\n",
    "    for i in range(0,3):\n",
    "        url_tags=driver.find_elements_by_xpath(\"//div[@class='islrc']//div[@class='isv-r PNCib MSM1fd BUooTd']//a[@class='VFACy kGQAp sMi44c lNHeqe WGvvNb']\")\n",
    "        for i in url_tags:\n",
    "            cars.append(i.get_attribute('href'))   \n",
    "        time.sleep(2)\n",
    "#extracting machine learning imgae data        \n",
    "    driver.get(url)\n",
    "    search_field_items=driver.find_element_by_xpath(\"//div[@class='a4bIc']/input\")\n",
    "    search_field_items.send_keys(\"machine learning\")\n",
    "    search_button=driver.find_element_by_xpath(\"//button[@class='Tg7LZd']\")\n",
    "    search_button.click()\n",
    "    for i in range(0,3):\n",
    "        url_tags=driver.find_elements_by_xpath(\"//div[@class='islrc']//div[@class='isv-r PNCib MSM1fd BUooTd']//a[@class='VFACy kGQAp sMi44c lNHeqe WGvvNb']\")\n",
    "        for i in url_tags:\n",
    "            machine_learning.append(i.get_attribute('href'))   \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    print('Length of the list are ==>>',len(fruits),len(cars),len(machine_learning))\n",
    "    google_image=pd.DataFrame({'Bikes':Bikes[:100],\n",
    "                           'cars':cars[:100],\n",
    "                            'machine_learning':machine_learning[:100]})\n",
    "                            \n",
    "    print(google_image)\n",
    "    google_image.to_csv('google_image.csv', index = False)\n",
    "    \n",
    "    \n",
    "# Calling Function\n",
    "google_img('https://images.google.com/?gws_rd=ssl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import requests\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter City Name : Bhubaneswar\n",
      "URL Extracted:  https://www.google.co.in/maps/place/Bhubaneswar,+Odisha/@20.3168274,85.8178939,15z/data=!4m5!3m4!1s0x3a1909d2d5170aa5:0xfc580e2b68b33fa8!8m2!3d20.2960587!4d85.8245398\n",
      "Latitude = 20.3168274, Longitude = 85.8178939\n"
     ]
    }
   ],
   "source": [
    "# opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "city = input('Enter City Name : ')                                       \n",
    "search = driver.find_element_by_id(\"searchboxinput\")                    \n",
    "search.clear()                                                             \n",
    "time.sleep(2)\n",
    "search.send_keys(city)                                                    \n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")              \n",
    "button.click()                                                          \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Q6.Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import requests\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://trak.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting Search Button.\n",
    "search_button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(search_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating Empty List:\n",
    "trade_dict = {}\n",
    "trade_dict['Date'] = []\n",
    "trade_dict['Startup Name'] = []\n",
    "trade_dict['Industry/Vertical'] = []\n",
    "trade_dict['Sub-Vertical'] = []\n",
    "trade_dict['Location'] = []\n",
    "trade_dict['Investor'] = []\n",
    "trade_dict['Investment Type'] = []\n",
    "trade_dict['Amount(in USD)'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>M&amp;A</td>\n",
       "      <td>1,200,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Vedantu</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Coatue Management</td>\n",
       "      <td>Series D</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Crio</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Learning Platform for Developers</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>021 Capital</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>934,160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>goDutch</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Group Payments</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Matrix India,Y Combinator, Global Founders Cap...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Mystifly</td>\n",
       "      <td>Airfare Marketplace</td>\n",
       "      <td>Ticketing, Airline Retailing, and Post-Ticketi...</td>\n",
       "      <td>Singapore and Bangalore</td>\n",
       "      <td>Recruit Co. Ltd.</td>\n",
       "      <td>pre-Series B</td>\n",
       "      <td>3,300,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Startup Name    Industry/Vertical  \\\n",
       "0  15/07/2020     Flipkart           E-commerce   \n",
       "1  16/07/2020      Vedantu              EduTech   \n",
       "2  16/07/2020         Crio              EduTech   \n",
       "3  14/07/2020      goDutch              FinTech   \n",
       "4  13/07/2020     Mystifly  Airfare Marketplace   \n",
       "\n",
       "                                        Sub-Vertical                 Location  \\\n",
       "0                                         E-commerce                Bangalore   \n",
       "1                                    Online Tutoring                Bangalore   \n",
       "2                   Learning Platform for Developers                Bangalore   \n",
       "3                                     Group Payments                   Mumbai   \n",
       "4  Ticketing, Airline Retailing, and Post-Ticketi...  Singapore and Bangalore   \n",
       "\n",
       "                                            Investor Investment Type  \\\n",
       "0                                        Walmart Inc             M&A   \n",
       "1                                  Coatue Management        Series D   \n",
       "2                                        021 Capital    pre-Series A   \n",
       "3  Matrix India,Y Combinator, Global Founders Cap...            Seed   \n",
       "4                                   Recruit Co. Ltd.    pre-Series B   \n",
       "\n",
       "  Amount(in USD)  \n",
       "0  1,200,000,000  \n",
       "1    100,000,000  \n",
       "2        934,160  \n",
       "3      1,700,000  \n",
       "4      3,300,000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(40,51):\n",
    "    driver.find_element_by_xpath('//div[@id=\"tablepress-{}_wrapper\"]/div/label/select/option[4]'.format(i)).click()\n",
    "\n",
    "    # Date\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "trade_df = pd.DataFrame(trade_dict)\n",
    "trade_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Write a program to scrap all the available details of top 10 gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import requests\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.digit.in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on Top 10 Option: \n",
    "top_10 = driver.find_element_by_xpath(\"//div[@class='menu']/ul/li[3]/a\")\n",
    "top_10.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the Laptop options:\n",
    "laptops = driver.find_element_by_xpath(\"//ul[@id='top10list']/li[2]\")\n",
    "laptops.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the Gaming Laptops:\n",
    "gaming_laptop = driver.find_element_by_xpath(\"//div[@id='laptops']/a[3]\")\n",
    "gaming_laptop.get(gaming_laptop.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Empty List\n",
    "Laptop_name = []\n",
    "price = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "RAM = []\n",
    "weight = []\n",
    "dimension = []\n",
    "GPU = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the tags having computer Names:\n",
    "names = driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    if i.text is None:\n",
    "        Laptop_name.append(\" \") \n",
    "    else:\n",
    "        Laptop_name.append(i.text.replace('\\n','').strip())\n",
    "        \n",
    "#operating System:\n",
    "os = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    if i.text is None:\n",
    "        OS.append(\" \") \n",
    "    else:\n",
    "        OS.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#display\n",
    "displays = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    if i.text is None:\n",
    "        display.append(\" \") \n",
    "    else:\n",
    "        display.append(i.text.replace('\\n','').strip())\n",
    "    \n",
    "#processor\n",
    "processors = driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[3]/div/div\")\n",
    "for i in processors:\n",
    "    if i.text is None:\n",
    "        processor.append(\" \") \n",
    "    else:\n",
    "        processor.append(i.text.replace('\\n','').strip())\n",
    "\n",
    "#memory\n",
    "memories = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[1]\")\n",
    "memories_specification = driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "for i in range(len(memories)):\n",
    "        if memories[i].text=='Memory':\n",
    "            HDD.append(memories_specification[i].text.split('/')[0])\n",
    "            RAM.append(memories_specification[i].text.split('/')[1])\n",
    "        else:\n",
    "            HDD.append('No details found')\n",
    "            RAM.append('No details found') \n",
    "\n",
    "#weight\n",
    "weights = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")\n",
    "weight_spec=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")\n",
    "for i in range(len(weights)):\n",
    "        if weights[i].text=='Weight':\n",
    "            weight.append(weight_spec[i].text)\n",
    "        \n",
    "#dimension\n",
    "dimension = []\n",
    "dimens = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")#list of specificaion name\n",
    "dims_spec = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")#values of specifiations\n",
    "for i in range(len(dimens)):\n",
    "        if dimens[i].text=='Dimension':\n",
    "            dimension.append(dimens_spec[i].text)\n",
    "\n",
    "#GPU Driver:\n",
    "GPUs = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")\n",
    "GPUs_spec = driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")\n",
    "for i in range(len(GPUs)):\n",
    "        if GPUs[i].text == 'Graphics Processor':\n",
    "            GPU.append(GPUs_spec[i].text)\n",
    "        \n",
    "full_specs = []\n",
    "urls = driver.find_elements_by_xpath(\"//div[@class='full-specs']/span\")\n",
    "for i in urls:\n",
    "    if i.get_attribute('data-href'):\n",
    "        full_specs.append(i.get_attribute('data-href'))\n",
    "    \n",
    "for i in full_specs:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        prices=driver.find_element_by_xpath(\"//div[@class='Block-price']/b\")\n",
    "        price.append(prices.text)\n",
    "    except NoSuchElementException:\n",
    "        price.append(\"No details available\")\n",
    "        \n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"Price\":price,\n",
    "                \"OS\":OS,\n",
    "                \"Display\":display,\n",
    "                \"HDD\":HDD,\n",
    "                 \"RAM\":RAM,\n",
    "                \"processor\":processor,\n",
    "                \"weight\":weight,\n",
    "                \"Dimension\":dimension,\n",
    "                \"Graphical processor\":GPU})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Gaming laptops_digit.csv\") #Saving the File"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
