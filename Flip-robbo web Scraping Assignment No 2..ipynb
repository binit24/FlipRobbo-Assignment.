{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details such as Skill,Designation and Location:\n",
    "field_designation = driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "field_location = driver.find_element_by_id('qsb-location-sugg')\n",
    "field_designation.send_keys(\"Data Analyst\")\n",
    "field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the search button:\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists \n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_needed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-titles:\n",
    "titles = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "#Using for loop for iterations\n",
    "#Handling Null Values.\n",
    "for i in titles:\n",
    "    if i.text is None:\n",
    "        job_title.append(\" \") \n",
    "    else:\n",
    "        job_title.append(i.text.replace('\\n','').strip())\n",
    "print(job_title[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-location:\n",
    "locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in locations:\n",
    "    if i.text is None:\n",
    "        job_location.append(\" \") \n",
    "    else:\n",
    "        job_location.append(i.text.replace('\\n','').strip())\n",
    "print(job_location[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_name:\n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\" \") \n",
    "    else:\n",
    "        company_name.append(i.text.replace('\\n','').strip())\n",
    "print(company_name[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for experience_required:\n",
    "experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in experience:\n",
    "    if i.text is None :\n",
    "            experience_required.append(\" \") \n",
    "    else:\n",
    "        experience_needed.append(i.text.replace('\\n','').strip())\n",
    "print(experience_needed[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['company names'] = company_name[0:10]\n",
    "df['job locations'] = job_location[0:10]\n",
    "df['experience required'] = experience_needed[0:10]\n",
    "df['job titles'] = job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details such as Skill,Designation and Location:\n",
    "field_designation = driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "field_location = driver.find_element_by_id('qsb-location-sugg')\n",
    "field_designation.send_keys(\"Data Scientist\")\n",
    "field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the search button for obtaining result:\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists \n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "job_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-titles:\n",
    "titles = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "#Using for loop for iterations.\n",
    "#Handling Null Values.\n",
    "for i in titles:\n",
    "    if i.text is None:\n",
    "        job_title.append(\" \") \n",
    "    else:\n",
    "        job_title.append(i.text.replace('\\n','').strip())\n",
    "print(job_title[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-location:\n",
    "locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in locations:\n",
    "    if i.text is None:\n",
    "        job_location.append(\" \") \n",
    "    else:\n",
    "        job_location.append(i.text.replace('\\n','').strip())\n",
    "print(job_location[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_name:\n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\" \") \n",
    "    else:\n",
    "        company_name.append(i.text.replace('\\n','').strip())\n",
    "print(company_name[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the full job-description,for scraping full job description we have to go in each of the jobs separately\n",
    "urls=[i.get_attribute(\"href\")for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")]\n",
    "for url in urls[0:10]:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        description = driver.find_element_by_xpath(\"//section[@class='job-desc']/div[1]\").text\n",
    "        for i in description:\n",
    "            job_description.append(i.text.replace('\\n1','\\n','\\n\\n',' ').strip('\\n'))   \n",
    "    except NoSuchElementException:\n",
    "        job_description.append(\" \")\n",
    "print(job_description[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['company names'] = company_name[0:10]\n",
    "df['job locations'] = job_location[0:10]\n",
    "df['job titles'] = job_title[0:10]\n",
    "df['job_description'] = job_description[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details such as Skill,Designation and Location:\n",
    "field_designation = driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "field_location = driver.find_element_by_id('qsb-location-sugg')\n",
    "field_designation.send_keys(\"Data Scientist\")\n",
    "field_location.send_keys(\"Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the search button for obtaining result:\n",
    "search_button = driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists \n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_needed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the tags for location textbox:\n",
    "location = driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\") #We are taking title name.\n",
    "#clicking the check box\n",
    "location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the tags for Salary textbox:\n",
    "Salary = driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\") #We are taking title name.\n",
    "#clicking the check box\n",
    "Salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-titles:\n",
    "titles = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "\n",
    "#Using for loop for iterations\n",
    "#Handling Null Values.\n",
    "for i in titles:\n",
    "    if i.text is None:\n",
    "        job_title.append(\" \") \n",
    "    else:\n",
    "        job_title.append(i.text.replace('||',' ').strip('\\n'))\n",
    "print(job_title[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for experience_required:\n",
    "experience = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in experience:\n",
    "    if i.text is None :\n",
    "            experience_required.append(\" \") \n",
    "    else:\n",
    "        experience_needed.append(i.text.replace('\\n','').strip('\\n'))\n",
    "print(experience_needed[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_name:\n",
    "companies = driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\" \") \n",
    "    else:\n",
    "        company_name.append(i.text.replace('\\n','').strip())\n",
    "print(company_name[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags for job-location:\n",
    "locations = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in locations:\n",
    "    if i.text is None:\n",
    "        job_location.append(\" \") \n",
    "    else:\n",
    "        job_location.append(i.text.replace('\\n','').strip())\n",
    "print(job_location[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['company names'] = company_name[0:10]\n",
    "df['job locations'] = job_location[0:10]\n",
    "df['experience required'] = experience_needed[0:10]\n",
    "df['job titles'] = job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.glassdoor.co.in/index.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium WebDriverWait is one of the Explicit waits. Explicit waits are confined to a particular web element. Explicit Wait is code you define to wait for a certain condition to occur before proceeding further in the code. Explicit wait is of two types: WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries.\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details in Blank boxes by using WebDriverWait:\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='sc.keyword']\"))).send_keys(\"Data Scientist\")\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='sc.location']\"))).send_keys(\"Noida India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cliking on search button.\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//button[@data-test='search-bar-submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list:\n",
    "company_name = []\n",
    "company_rating = []\n",
    "No_of_days_ago = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_name:\n",
    "companies = driver.find_elements_by_xpath(\"//a[@class=' css-10l5u4p e1n63ojh0 jobLink']/span\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\" \") \n",
    "    else:\n",
    "        company_name.append(i.text.replace('\\n','').strip())\n",
    "print(company_name[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_ratings:\n",
    "rating = driver.find_elements_by_xpath(\"//span[@class='compactStars ']\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in rating:\n",
    "    if i.text is None :\n",
    "        company_rating.append(\" \") \n",
    "    else:\n",
    "        company_rating.append(i.text.replace('\\n','').strip())\n",
    "print(company_rating[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags where no of days ago job was posted:\n",
    "job_posted = driver.find_elements_by_xpath(\"//div[@data-test='job-age']\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in job_posted:\n",
    "    if i.text is None :\n",
    "        No_of_days_ago.append(\" \") \n",
    "    else:\n",
    "        No_of_days_ago.append(i.text[0])\n",
    "print(No_of_days_ago[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['company names'] = company_name[0:10]\n",
    "df['company_rating'] = company_rating[0:10]\n",
    "df['No_of_days_ago'] = No_of_days_ago[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.glassdoor.co.in/Salaries/index.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries.\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details in Blank boxes by using WebDriverWait:\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='KeywordSearch']\"))).send_keys(\"Data Scientist\")\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='LocationSearch']\"))).send_keys(\"Noida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cliking on search button.\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//button[@data-test='search-bar-submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list:\n",
    "company_Name = []\n",
    "minimum_salaries = []\n",
    "average_salaries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags company_name:\n",
    "companies = driver.find_elements_by_xpath(\"//div[@data-test='job-info']/p[2]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_Name.append(\" \") \n",
    "    else:\n",
    "        company_Name.append(i.text.replace('\\n','').strip())\n",
    "print(company_Name[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags of average Salary:\n",
    "avg_salary = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in avg_salary:\n",
    "    if i.text is None :\n",
    "        average_salaries.append(\" \") \n",
    "    else:\n",
    "        average_salaries.append(i.text.replace('\\n','').strip())\n",
    "print(average_salaries[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags of minimum Salary:\n",
    "minimum_salary = driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in minimum_salary:\n",
    "    if i.text is None :\n",
    "        minimum_salaries.append(\" \") \n",
    "    else:\n",
    "        minimum_salaries.append(i.text.replace('\\n','').strip())\n",
    "print(minimum_salaries[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['company names'] = company_Name[0:10]\n",
    "df['minimum_salaries'] = minimum_salaries[0:10]\n",
    "df['average_salaries'] = average_salaries[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element_by_class_name('_3704LK')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the button and clicking it toh search for sunglasses\n",
    "button=driver.find_element_by_class_name('L0Z3Pu')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand_names = []\n",
    "price = []\n",
    "description = []\n",
    "discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags for names.\n",
    "for i in soup.find_all('div', attrs ={'class':'_2WkVRV'}):\n",
    "    string = i.text\n",
    "    brand_names.append(string.strip())\n",
    "print(brand_names[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags for Prices.\n",
    "for i in soup.find_all('div', attrs ={'class':'_30jeq3'}):\n",
    "    string = i.text\n",
    "    price.append(string.strip())\n",
    "print(price[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags discounts.\n",
    "for i in soup.find_all('div', attrs ={'class':'_3Ay6Sb'}):\n",
    "    string = i.text\n",
    "    discount.append(string.strip())\n",
    "print(discount[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using for loop and fetching tags discounts.\n",
    "for i in soup.find_all('div', attrs ={'class':'_1xHGtK _373qXS'}):\n",
    "    string = i.text\n",
    "    description.append(string.replace('\\n','').strip())\n",
    "list(description[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['brand_names'] = brand_names[0:10]\n",
    "df['price'] = price[0:10]\n",
    "df['discount'] = discount[0:10]\n",
    "df['description'] = description[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "WEB SCRAPING ASSIGNMENT-2\n",
    ".\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the details such as Skill,Designation and Location:\n",
    "field_designation = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "field_designation.clear() \n",
    "field_designation.send_keys(\"laptops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the search button for obtaining result:\n",
    "search_button = driver.find_element_by_xpath('//span[@id=\"nav-search-submit-text\"]') \n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to locate the tags for core i7:\n",
    "i7_filter_button = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in i7_filter_button:\n",
    "    if i.text == 'Intel Core i7':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to locate the tags for core i9 features:\n",
    "i9_filter_button = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in i9_filter_button:\n",
    "    if i.text == 'Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "laptop_titles = []\n",
    "laptop_ratings = []\n",
    "laptop_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags of titles:\n",
    "titles = driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        laptop_titles.append(\" \") \n",
    "    else:\n",
    "        laptop_titles.append(i.text.replace('\\n',' ').strip('/n'))\n",
    "print(laptop_titles[6:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the tags of prices:\n",
    "prices = driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "\n",
    "#Using for loop for iterations:\n",
    "#Handling Null Values.\n",
    "for i in prices:\n",
    "    if i.text is None :\n",
    "        laptop_price.append(\" \") \n",
    "    else:\n",
    "        laptop_price.append(i.text.replace('\\n',' ').strip('/n'))\n",
    "print(laptop_price[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the full Laptop ratings for all the laptops.\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "urls = driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "get_URL = []\n",
    "for i in urls[:10]:\n",
    "    get_URL.append(i.get_attribute('href'))\n",
    "for url in get_URL:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rate=driver.find_element_by_xpath(\"//span[@id='acrCustomerReviewText']\")\n",
    "        rate.click()                                                      \n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "        laptop_ratings.append(rating.text.replace(' ',' ').strip('/n'))   \n",
    "    except NoSuchElementException:\n",
    "        laptop_ratings.append(\" \")\n",
    "print(laptop_ratings[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({})\n",
    "df['laptop_titles'] = laptop_titles[0:10]\n",
    "df['laptop_price'] = laptop_price[0:10]\n",
    "df['laptop_ratings'] = laptop_ratings[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking head and Shape:\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8:Scrape data for first 100 sneakers you find when you visit flipkart.comand search for “sneakers” in the search field.You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required Libraries:\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\MI Laptop\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.flipkart.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opeaning the web page through our web driver:\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element_by_class_name('_3704LK')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys('sneakers')\n",
    "button=driver.find_element_by_class_name('L0Z3Pu')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand = []\n",
    "description = []\n",
    "price = []\n",
    "discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the required details\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 3 page\n",
    "    brands=driver.find_elements_by_class_name('_2B_pmu')\n",
    "    for i in brands:\n",
    "        brand.append(i.text)\n",
    "    desc=driver.find_elements_by_class_name('_2mylT6')\n",
    "    for i in desc:\n",
    "        description.append(i.get_attribute('title'))\n",
    "    prices=driver.find_elements_by_xpath(\"//div[@class='_1vC4OE']\")\n",
    "    for i in prices[:40]:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements_by_xpath(\"//div[@class='VGWI6T']\")\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_2Xp0TH']\")\n",
    "    driver.get(nxt_button[page].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
